## The Move to the Ocean

> “You must be shapeless, formless, like water.” - Bruce Lee

With the new technologies in mind, back to "Video is Software" and the problems I described in my research to address and reference content within my linear, legacy applications and documents.

I experienced a lot of copying throughout the process. From the web, to "Notes", to "Notion", to post-its, to cards, to "After Effects", to "InDesign", to "Premiere" and to this document here. And once the notes or quotes were in there, I have to keep track of changes within each text element, document and project file. And this is insane!

> My notes are not compatible with the tools and the tools are not compatible with the notes and the internet.

But maybe it is now a good time to ask: How does web-archiving work currently? And interestingly, it functions very similar to the processes how IPFS works. If [archive.org](https://archive.org/) for example crawls the web for new versions a website, they don't compare the words. They use the hashing algorithm for example for the whole site and within that for example each paragraph to check if the hash is consistent with the last saved version in their archive. This way they don't need to compute the specific content but only "sensing" the differences and where they appear. If that is the case, a new version of the website exist and the archive can be updated.

So now both sides seem to match up: if web-archiving works this way and new content creation and identification in IPFS works this way, doesn't that mean that if we change our approach just a little bit, we will end up constructing files that are already meant to be archivable and uniquely referential in the first place? Now everything starts to flow.

Back to my first Twitter prototype in April 2018 and what this experiment might mean now. As I described earlier, I deconstructed and with that "archived" interesting parts of bigger articles within the available browser interface. But in the end, it is a comparable approach to the methods web-archiving would use: One Tweet contained one snippet of information that would be now extendable to add further notes via the reply or comment function as well as addressable because the Tweet has now been indexed, and not hashed, with its own "content address" with Twitter as the "content manager" and "address book" because the service is location-based and not content-based addressing.

![Content Deconstruction with Twitter Interface](/Users/xr/Documents/VERTICAL/Vertical-One/assets/twitter-blocks.png)

If I now look at Twitter's core functions of micro-blogging, tweet-threading and its network-specific referencing system we can iterate upon that concept and generalize it:
Micro-blogging then becomes "mirco-blocking", independent blocks that you can arrange in sequences, but are specifically general purpose and not application or network dependent, only on the protocol level.
With this perspective the "hot topic" of "blockchain technology" becomes more like a "block-chaining practice" in itself.

Furthermore, my analog processes and tools underline this concept of atomization and working with this kind of self-describing files, especially in explorative research on the web and while dealing with lots of uncertainties.   

If this is then a native method for the internet, then I come to the conclusion that none of the existing applications and tools are really anticipating these potential capabilities. These applications are mostly offline, desktop-based and specific purpose and want to be used for that purpose only. For those, the internet exists only after "export".

The most open tool and specifically developed for the internet is obviously the browser itself. So how can legacy word processors, editing tools, etc. become more IPFS-capable and the IPFS-capable browser more like editing software?

And this is the next rabbit hole I want to follow:
We already have powerful browsers. We already have "browser-friendly" codes to stream videos, thank you Google. And we now have an online filesystem, thank you IPFS. Now we need tools for creating reusable, block-based (thank you Notion for starting), protocol-specific assets, tools for content management (thank you KirbyCMS for adopting), as well as a tool for composition.  

The result of this information architecture will be a form of "responsive performance" of micro-content and micro-services. Designs can be defined relative in terms of its aspect ratios and sizes, we have already seen it in the evolution of responsive websites. The missing part of this framework is only: how to handle the timing of each content? That's it. That is the only piece missing in this puzzle:
a playback engine for "multi-media" micro-contents within the browser.
