### Functional Referencing
// Archiving, Time-Stamping, Hashing, Referencing, Building Recipes..

In this chapter I want to layout the problems of current functional referencing and how media archeology or internet-based research might deal with it. And it is all based on one important realization:

**Video is software, therefore treat video as software.**

To not realize this fact is probably the single biggest misconception we have in "new media" today, focussing more on what happens inside the the frame, on the final rendered content itself, but not how the software actually shapes it. And I think that Lev Manovich identifies this problem very well and this is the point I was trying to make in the "filesharing approaches" and "what the network wants": content on the platform is not merely hosted, it is actively shaped and formed specifically for and by the software for each platform.

> "Because all the new qualities of “digital media” are not situated “inside” the media objects. Rather, they all exist “outside” — as commands and techniques of media viewers, authoring software, animation, compositing, and editing software, game engine software, wiki software, and all other software 'species.'"
– Lev Manovich (Software Takes Command (2013), p. 149)

> "There is no such thing as “digital media.” There is only software — as applied to media (or “content”). Or, to put this differently: for users who only interact with media content through application software, the “properties” of digital media are defined by the particular software as opposed to solely being contained in the actual content (i.e., inside digital files)."
– Lev Manovich (Software Takes Command (2013), p. 152)


Throughout my case-study of mobile platform research (in the network attached media chapter) I could see very clear how much the video content is dependent on the software environment. But I took my video editing software for granted and couldn't make the switch to see the same pattern there. But I was already there, four years ago, when I started this whole journey.

In 2015, I worked on the project "Real-Time Propaganda", which was the first project of mine, that wasn't created by "traditional" video editing tools. Its content was downloaded by software, it was analyzed by software, it was edited and orchestrated by software and performed by software. The whole process was highly software dependent.  

This project did only exist because I used YouTube as a database, which would process the videos in such a degree that they would generate subtitles with speech-to-text software to make content discovery more precise.
Youtube still protects this „data-asset“ and it is still not part of the official API to request the "spoken content". With a workaround, I was now capable of accessing and gaining knowledge about the content without watching it and this links back to videos that are now machine readable!
And I would only realize it again when I build my first content-specific knowledge graph in the "Early Birds Series" (2018), that this is now the playground of AI and computer vision and not "click workers".

And it is important to understand that this "computability" not a video-specific property:

> "Visualization, searchability, findability — these and many other new “media-independent techniques” (i.e. concepts implemented to work across many data types)" clearly stand out in the map of the computer metamedium we have drawn because they go against our habitual understanding of media as plural (i.e. as consisting of a number of separate mediums). If we can use the same techniques across different media types, what happens to these distinctions between mediums?
– Lev Manovich (Software Takes Command (2013), p. 119)

When I look back to my approach to understand the methods of visual referencing, it looks primitive compared to the possibilities of software and enough computing power.

Because I only had the rendered version of "Transformers – The Premake" I had to deconstruct the used methods "by hand". The project is not recreate-able, it is application-dependent at a minimum, and close-source in terms of "re-exportablity" at a maximum. I would never be able to recreate the same project on my computer, or re-reference some asset within it. But on the computer of the editor, there is a project file. It is descriptive, it is the editor's actions translated into code and how the material is used, with all of the building blocks that are necessary.

I want to make the analogy to open source software and how they are dealing with "exporting".
Here we can make the distinction between the "repository" as the projects "online file-system" and the "releases", versions of executable software for different operating systems, the binaries. They are especially "non-text files", because they are meant for execution of the computer and not for "human-readability" or further editing.
That is where open source becomes important: the files from which the program is compiled from is open to editing, reviewing, extending, or simply copying. With that, you can build it yourself and create your own executable program. But what is even more valuable is that the patterns created in code can be used for different purposes and live as frameworks, snippets and building blocks within other projects.
This is why software is fast in development and desktop documentaries, video essays or evidence-based research is not. There is no asset sharing, no templates, no cooperation for the framework of composing different media content. We start from scratch with each project, we are rebuilding all the time. In contrast to "Real-Time Propaganda", which is completely bootstrapped out of multiple repositories of open source software, but recombined and linked together for this specific project.
The project wouldn't have happened, if I would have started from scratch.

Interestingly, I found a sticking statement from Forensic Architecture itself that also helps to frame the problem:  

> “Our aim is to develop new evidential techniques, so we kind of never do the same investigation twice or we never use the same methodologies twice. What we do, after we developed any software is we put it on the public domain, we put it as an open source code. Or if it is kind of techniques, more architectural or editing, image ways techniques, we have academies, we teach activists how to do it.”
– [Eyal Weizman (Forensic Architecture)](https://youtu.be/TrKM94YrEKA?t=2086)

In their practice, editing-based techniques can not be shared via open source code, but require an "expensive" form of small-group teaching of best practices and techniques. But when video is software, why aren't there any forms of "shared editing templates"?  







The “rendered” image hides its software influences, obfuscates its creation and is therefore still seen as a “separate“ medium.
the final rendering should only happen when “watching” but not when “working” with the material

new modes and possibilities for the video essay, that can embrace visuality of the discussed material itself

Video file formats are their own silos!
From silos and containers to (performant, interoperable) file-system repositories
video to sketch file
“Zipped and playable” or “unzipped and editable”
hidden in plain sight!


I thought the initial “research tool” was on the application layer, but actually it is one layer below, how files are structured and stored, prepared and edited. The “actual tool” is the self-describing structure itself which is then application independent and can be used by “open-tools”.


// What if you don't know if the project will become a video, a website, a PDF or a book? How do you then defer the decisions before it becomes a "product"? And what if it has to be cross-functional between those formats? How can this approach itself create "real collaboration" and not just a collection of "finished products", done separately?


Microblogging to MircoBlocking

Block collections become some sort of referencing /outlining tool for the long format


what are “browser-friendly” codecs to let the video assemble itself? > youtube: webm (VP8/9 + opus) > all streaming optimized (DASH) + wikipedia (VP9/8)
“Composition-friendly codecs for assets”


Now I point to the source with the link instead of “pinning” and therefore archiving for myself while providing it to others.
