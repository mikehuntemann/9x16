## Functional Referencing
In this chapter I want to layout the problems of current functional referencing and how media archeology or internet-based research has to deal with it. But first of all, we have to make one important realization:

**Video is software.**

To not realize this fact is probably the single biggest misconception we have in "new media" today, focussing more on what happens inside the the frame, on the final rendered content itself, but not how the software actually shapes it. And I think that Lev Manovich identifies this problem very well and this is the point I was trying to make in "filesharing approaches" and "what the network wants": content on the platform is not merely hosted, it is actively shaped and formed specifically for and by the software for each platform.

> "Because all the new qualities of “digital media” are not situated “inside” the media objects. Rather, they all exist “outside” — as commands and techniques of media viewers, authoring software, animation, compositing, and editing software, game engine software, wiki software, and all other software 'species.'"
– Lev Manovich (Software Takes Command (2013), p. 149)

The “rendered” image hides its software influences, obfuscates its creation process and is therefore still seen as a “separate“ medium.

> "There is no such thing as “digital media.” There is only software — as applied to media (or “content”). Or, to put this differently: for users who only interact with media content through application software, the “properties” of digital media are defined by the particular software as opposed to solely being contained in the actual content (i.e., inside digital files)."
– Lev Manovich (Software Takes Command (2013), p. 152)


Throughout my case-study of mobile platform research (in the network attached media chapter) I could see very clear how much the video content is dependent on the software environment. But I took my video editing software for granted and couldn't make the switch to see the same pattern there. But I was already there, four years ago, when I started this whole journey.

In 2015, I worked on the project "Real-Time Propaganda", which was the first project of mine, that wasn't created within my "traditional" video editing tools. Its content lives within software, it was downloaded by software, it was analyzed by software, it was edited and orchestrated by software and performed by software. The whole process was highly software dependent.  

This project did only exist because I used YouTube as a database, which would process the videos in such a degree that they would generate subtitles with speech-to-text software to make content discovery more precise.
Youtube still protects this „data-asset“ and it is still not part of the official API to request the "spoken content". With a workaround, I was now capable of accessing and gaining knowledge about the content without watching it and this links back to videos that are now machine readable!
And I would only realize it again when I build my first content-specific knowledge graph in the "Early Birds Series" (2018), that this is now the playground of AI and computer vision and not "click workers".

And it is important to understand that this "computability" not a video-specific property:

> "Visualization, searchability, findability — these and many other new “media-independent techniques” (i.e. concepts implemented to work across many data types)" clearly stand out in the map of the computer metamedium we have drawn because they go against our habitual understanding of media as plural (i.e. as consisting of a number of separate mediums). If we can use the same techniques across different media types, what happens to these distinctions between mediums?
– Lev Manovich (Software Takes Command (2013), p. 119)

When I look back to my approach to understand the methods of visual referencing, it looks primitive compared to the possibilities of software and enough computing power.

Because I only had the rendered version of "Transformers – The Premake" I had to deconstruct the used methods "by hand". The project is not recreate-able, it is application-dependent at a minimum, and close-source in terms of "re-exportablity" at a maximum. I would never be able to recreate the same project on my computer, or re-reference some asset within it. But on the computer of the editor, there is a project file. It is descriptive, it is the editor's actions translated into code and how the material is used, with all of the building blocks that are necessary.

I want to make the analogy to open source software and how they are dealing with "exporting".
Here we can make the distinction between the "repository" as the projects "online file-system" and the "releases", versions of executable software for different operating systems, the binaries. They are especially "non-text files", because they are meant for execution of the computer and not for "human-readability" or further editing.
That is where open source becomes important: the files from which the program is compiled from is open to editing, reviewing, extending, or simply copying. With that, you can build it yourself and create your own executable program. But what is even more valuable is that the patterns created in code can be used for different purposes and live as frameworks, snippets and building blocks within other projects.
This is why software is fast in development and desktop documentaries, video essays or evidence-based research is not. There is no asset sharing, no templates, no cooperation for the framework of composing different media content. We start from scratch with each project, we are rebuilding all the time. In contrast to "Real-Time Propaganda", which is completely bootstrapped out of multiple repositories of open source software, but recombined and linked together for this specific project.
The project wouldn't have happened, if I would have started from scratch.

Interestingly, I found a sticking statement from Forensic Architecture itself that also helps to frame the problem:  

> “Our aim is to develop new evidential techniques, so we kind of never do the same investigation twice or we never use the same methodologies twice. What we do, after we developed any software is we put it on the public domain, we put it as an open source code. Or if it is kind of techniques, more architectural or editing, image ways techniques, we have academies, we teach activists how to do it.”
– [Eyal Weizman (Forensic Architecture)](https://youtu.be/TrKM94YrEKA?t=2086)

In their practice, editing-based techniques can not be shared via open source code, but require an "expensive" form of small-group teaching of best practices and techniques. But when video is software, why aren't there any forms of "codified editing templates"?  

For now, the video file formats of the "final product" act as silos.
But what is necessary to break those silos down and replace them with "containers" to be as performant and interoperable as open source software repositories?

I thought the initial “research tool” was on the application layer, but actually it is one layer below, how files are structured and stored, prepared and edited. The “actual tool” is the self-describing structure itself which is then application independent and can be used by a collection of tools (like this document here).

And we can go beyond that and ask:
What if you don't even know if the project will become a video, a website, a PDF or a book – from a researcher perspective who is not directly bound to a specific medium in the first place but rather be the context of the research subject? As I described in the process of "TikTok Takeover", you don't even know what kind of content you potentially have to deal with. How do you then defer the decisions before it becomes a "product"? And further, what if it has to be cross-functional between multiple media formats and collaborators? How can this approach itself create "real collaboration" and not just a collection of "finished products", done separately?
